{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Demo of Ridge Regression using Stochasitc Gradient Descent optimization\n",
    "\n",
    "You will need to play with the `learning_rate` and `l2_lambda` (regularization parameter) hyperparameters in order to get a fast and stable optimizaiton.\n",
    "\n",
    "* Small batches allow us to get quick estimates of the true gradient (since we only have to process one batch worth of data, instead of the whole data set).\n",
    "\n",
    "* Randomization allows us to avoid pathalogical orderings of the data that might cause us to go off in the wrong direction.\n",
    "\n",
    "* If we are solving a non-convex problem (e.g. not this case), then randomization can also help to avoid and escape local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our minimization problem is\n",
    "$$\n",
    "\\min_\\theta \\mathcal{L}\n",
    "$$\n",
    "where our loss function is\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{m} \\|X\\theta - y\\|^2 + \\lambda\\|\\theta\\|^2\n",
    "$$\n",
    "\n",
    "The gradient is \n",
    "$$\n",
    "\\nabla_{\\theta} \\mathcal{L} = \\frac{2}{m} X^T (X\\theta - y) + 2\\lambda\\theta\n",
    "$$\n",
    "\n",
    "If we designate our error term as\n",
    "$$\n",
    "\\hat{\\varepsilon} = X\\theta - y\n",
    "$$\n",
    "and we drop the factor of 2 (since that can just be folded into the learning rate) then we have\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathcal{L} = \\frac{1}{m} X^T \\hat{\\varepsilon} + \\lambda\\theta\n",
    "$$\n",
    "\n",
    "This formulation corresponds directly to the implementations below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On CPU using Pandas\n",
    "\n",
    "Pandas + Numpy is often easiest for smaller problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_samples = 800\n",
    "n_features = 50_000\n",
    "\n",
    "# Features\n",
    "X = pd.DataFrame(np.random.randn(n_samples, n_features))\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "# True weights\n",
    "true_weights = np.random.randn(n_features)\n",
    "\n",
    "# Target variable with noise\n",
    "y = pd.Series(X.values @ true_weights + 0.1 * np.random.randn(n_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 35906.7735\n",
      "Epoch 2, Loss: 25504.9072\n",
      "Epoch 3, Loss: 18146.6122\n",
      "Epoch 4, Loss: 12933.8487\n",
      "Epoch 5, Loss: 9235.0068\n",
      "Epoch 6, Loss: 6604.7049\n",
      "Epoch 7, Loss: 4731.2204\n",
      "Epoch 8, Loss: 3395.0024\n",
      "Epoch 9, Loss: 2440.1197\n",
      "Epoch 10, Loss: 1756.6248\n",
      "Epoch 11, Loss: 1266.6852\n",
      "Epoch 12, Loss: 914.8037\n",
      "Epoch 13, Loss: 661.6896\n",
      "Epoch 14, Loss: 479.3859\n",
      "Epoch 15, Loss: 347.7772\n",
      "Epoch 16, Loss: 252.6872\n",
      "Epoch 17, Loss: 183.8635\n",
      "Epoch 18, Loss: 133.9741\n",
      "Epoch 19, Loss: 97.7534\n",
      "Epoch 20, Loss: 71.4237\n",
      "Epoch 21, Loss: 52.2548\n",
      "Epoch 22, Loss: 38.2787\n",
      "Epoch 23, Loss: 28.0762\n",
      "Epoch 24, Loss: 20.6184\n",
      "Epoch 25, Loss: 15.1595\n",
      "Epoch 26, Loss: 11.1595\n",
      "Epoch 27, Loss: 8.2246\n",
      "Epoch 28, Loss: 6.0678\n",
      "Epoch 29, Loss: 4.4817\n",
      "Epoch 30, Loss: 3.3136\n",
      "Epoch 31, Loss: 2.4525\n",
      "Epoch 32, Loss: 1.8171\n",
      "Epoch 33, Loss: 1.3476\n",
      "Epoch 34, Loss: 1.0003\n",
      "Epoch 35, Loss: 0.7433\n",
      "Epoch 36, Loss: 0.5529\n",
      "Epoch 37, Loss: 0.4116\n",
      "Epoch 38, Loss: 0.3067\n",
      "Epoch 39, Loss: 0.2288\n",
      "Epoch 40, Loss: 0.1708\n",
      "Epoch 41, Loss: 0.1277\n",
      "Epoch 42, Loss: 0.0955\n",
      "Epoch 43, Loss: 0.0715\n",
      "Epoch 44, Loss: 0.0536\n",
      "Epoch 45, Loss: 0.0402\n",
      "Epoch 46, Loss: 0.0302\n",
      "Epoch 47, Loss: 0.0227\n",
      "Epoch 48, Loss: 0.0171\n",
      "Epoch 49, Loss: 0.0129\n",
      "Epoch 50, Loss: 0.0098\n",
      "Training completed in 2.07 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros(n_features)\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Timing start\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "  # Shuffle the data\n",
    "  indices = np.random.permutation(n_samples)\n",
    "  X_shuffled = X.values[indices]\n",
    "  y_shuffled = y.values[indices]\n",
    "\n",
    "  for i in range(0, n_samples, batch_size):\n",
    "    X_batch = X_shuffled[i:i+batch_size]\n",
    "    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "    # Predict\n",
    "    y_pred = X_batch @ weights\n",
    "\n",
    "    # Compute error\n",
    "    error = y_pred - y_batch\n",
    "\n",
    "    l2_lambda = 1e-3\n",
    "\n",
    "    # Compute gradient\n",
    "    gradient = X_batch.T @ error / batch_size + l2_lambda * weights\n",
    "\n",
    "    # Update weights\n",
    "    weights -= learning_rate * gradient\n",
    "\n",
    "  # Compute loss\n",
    "  y_full_pred = X.values @ weights\n",
    "  loss = np.mean((y_full_pred - y.values) ** 2)\n",
    "  print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Timing end\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by SHAP / feature importance\n",
      "47415    0.444863\n",
      "29437    0.408179\n",
      "23284    0.405233\n",
      "45375    0.380249\n",
      "9019     0.378597\n",
      "14792    0.376864\n",
      "32212    0.370889\n",
      "41712    0.370864\n",
      "24048    0.365077\n",
      "37256    0.364913\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_mean = X.mean(axis=0).values\n",
    "feature_contributions = (X - X_mean) * weights  # shape (n_samples, n_features)\n",
    "\n",
    "print(\"Top 10 features by SHAP / feature importance\")\n",
    "print(\n",
    "  feature_contributions\n",
    "  .abs()\n",
    "  .mean(axis=0)\n",
    "  .sort_values(ascending=False).head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch\n",
    "\n",
    "Pytorch makes it really easy to parallize onto GPUs or using the parallel processing capabilities of Apple processors (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1, Loss: 23782.8242\n",
      "Epoch 2, Loss: 11265.2275\n",
      "Epoch 3, Loss: 5383.1626\n",
      "Epoch 4, Loss: 2593.4277\n",
      "Epoch 5, Loss: 1260.0693\n",
      "Epoch 6, Loss: 617.2495\n",
      "Epoch 7, Loss: 304.6308\n",
      "Epoch 8, Loss: 151.4463\n",
      "Epoch 9, Loss: 75.8233\n",
      "Epoch 10, Loss: 38.2100\n",
      "Epoch 11, Loss: 19.3741\n",
      "Epoch 12, Loss: 9.8788\n",
      "Epoch 13, Loss: 5.0659\n",
      "Epoch 14, Loss: 2.6123\n",
      "Epoch 15, Loss: 1.3531\n",
      "Epoch 16, Loss: 0.7045\n",
      "Epoch 17, Loss: 0.3684\n",
      "Epoch 18, Loss: 0.1935\n",
      "Epoch 19, Loss: 0.1022\n",
      "Epoch 20, Loss: 0.0542\n",
      "Epoch 21, Loss: 0.0289\n",
      "Epoch 22, Loss: 0.0155\n",
      "Epoch 23, Loss: 0.0084\n",
      "Epoch 24, Loss: 0.0045\n",
      "Epoch 25, Loss: 0.0025\n",
      "Epoch 26, Loss: 0.0014\n",
      "Epoch 27, Loss: 0.0008\n",
      "Epoch 28, Loss: 0.0005\n",
      "Epoch 29, Loss: 0.0003\n",
      "Epoch 30, Loss: 0.0002\n",
      "Epoch 31, Loss: 0.0001\n",
      "Epoch 32, Loss: 0.0001\n",
      "Epoch 33, Loss: 0.0001\n",
      "Epoch 34, Loss: 0.0000\n",
      "Epoch 35, Loss: 0.0000\n",
      "Epoch 36, Loss: 0.0000\n",
      "Epoch 37, Loss: 0.0000\n",
      "Epoch 38, Loss: 0.0000\n",
      "Epoch 39, Loss: 0.0000\n",
      "Epoch 40, Loss: 0.0000\n",
      "Epoch 41, Loss: 0.0000\n",
      "Epoch 42, Loss: 0.0000\n",
      "Epoch 43, Loss: 0.0000\n",
      "Epoch 44, Loss: 0.0000\n",
      "Epoch 45, Loss: 0.0000\n",
      "Epoch 46, Loss: 0.0000\n",
      "Epoch 47, Loss: 0.0000\n",
      "Epoch 48, Loss: 0.0000\n",
      "Epoch 49, Loss: 0.0000\n",
      "Epoch 50, Loss: 0.0000\n",
      "Training completed in 3.10 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timeit\n",
    "\n",
    "# Select device (GPU if available, else CPU)\n",
    "device = (\n",
    "  torch.device(\"cuda\")\n",
    "  if torch.cuda.is_available()\n",
    "  else torch.device(\"mps\")\n",
    "  if torch.backends.mps.is_available()\n",
    "  else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "X_tensor = torch.tensor(X.values.astype(np.float32), device=device)\n",
    "y_tensor = torch.tensor(y.values.astype(np.float32), device=device)\n",
    "\n",
    "# Initialize model parameters\n",
    "weights = torch.zeros(n_features, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "l2_lambda = 1e-3\n",
    "\n",
    "# Timing start\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "  perm = torch.randperm(n_samples, device=device)\n",
    "  X_shuffled = X_tensor[perm]\n",
    "  y_shuffled = y_tensor[perm]\n",
    "\n",
    "  for i in range(0, n_samples, batch_size):\n",
    "    X_batch = X_shuffled[i:i+batch_size]\n",
    "    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "    y_pred = X_batch @ weights\n",
    "    error = y_pred - y_batch\n",
    "\n",
    "    loss = (error ** 2).mean() + l2_lambda * (weights ** 2).sum()\n",
    "\n",
    "    # Backpropagation - this calculates the gradients for all the tensors with requires_grad=True\n",
    "    loss.backward()\n",
    "\n",
    "    # SGD step - this is where we update the weights\n",
    "    with torch.no_grad():\n",
    "      weights -= learning_rate * weights.grad\n",
    "      weights.grad.zero_()\n",
    "\n",
    "  # Compute full loss\n",
    "  with torch.no_grad():\n",
    "    y_full_pred = X_tensor @ weights\n",
    "    full_loss = ((y_full_pred - y_tensor) ** 2).mean().item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {full_loss:.4f}\")\n",
    "\n",
    "# Timing end\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALthough the total time is longer, it converges much faster (presumably because it uses better gradients), so if we were stopping on an epsilon, it would probably be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by SHAP / feature importance\n",
      "47415    0.445028\n",
      "29437    0.408296\n",
      "23284    0.405386\n",
      "45375    0.380387\n",
      "9019     0.378700\n",
      "14792    0.377018\n",
      "32212    0.371025\n",
      "41712    0.370977\n",
      "24048    0.365115\n",
      "37256    0.365078\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "X_tensor_mean = X_tensor.mean(axis=0)\n",
    "feature_contributions_torch = (X_tensor - X_tensor_mean) * weights  # shape (n_samples, n_features)\n",
    "\n",
    "print(\"Top 10 features by SHAP / feature importance\")\n",
    "print(\n",
    "  pd.Series(\n",
    "    feature_contributions_torch\n",
    "    .abs()\n",
    "    .mean(axis=0)\n",
    "    .cpu()\n",
    "    .detach()\n",
    "    .numpy()\n",
    "  )\n",
    "  .sort_values(ascending=False)\n",
    "  .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pseudo inverse via SVD with L2 regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss (SVD pseudo-inverse): 0.0000\n",
      "Training completed in 4.18 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Timing start\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Compute SVD\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "# Compute pseudo-inverse of the singular values\n",
    "l2_lambda = 1e-3  # Same as other methods\n",
    "S_inv = np.diag(S / (S**2 + l2_lambda))  # Regularized version\n",
    "\n",
    "# Compute pseudo-inverse of X\n",
    "X_pinv = Vt.T @ S_inv @ U.T\n",
    "\n",
    "# Solve for weights\n",
    "w_svd = X_pinv @ y\n",
    "\n",
    "# Timing end\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Check training loss\n",
    "y_pred = X @ w_svd\n",
    "loss = np.mean((y_pred - y) ** 2)\n",
    "print(f\"Training Loss (SVD pseudo-inverse): {loss:.4f}\")\n",
    "\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by SHAP / feature importance\n",
      "47415    0.445034\n",
      "29437    0.408301\n",
      "23284    0.405391\n",
      "45375    0.380392\n",
      "9019     0.378705\n",
      "14792    0.377023\n",
      "32212    0.371031\n",
      "41712    0.370982\n",
      "24048    0.365119\n",
      "37256    0.365084\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "feature_contributions_svd = (X - X_mean) * w_svd  # shape (n_samples, n_features)\n",
    "\n",
    "print(\"Top 10 features by SHAP / feature importance\")\n",
    "print(\n",
    "  feature_contributions_svd\n",
    "  .abs()\n",
    "  .mean(axis=0)\n",
    "  .sort_values(ascending=False).head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
