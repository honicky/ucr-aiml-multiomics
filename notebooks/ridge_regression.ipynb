{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Demo of Ridge Regression using Stochasitc Gradient Descent optimization\n",
    "\n",
    "You will need to play with the `learning_rate` and `l2_lambda` (regularization parameter) hyperparameters in order to get a fast and stable optimizaiton.\n",
    "\n",
    "* Small batches allow us to get quick estimates of the true gradient (since we only have to process one batch worth of data, instead of the whole data set).\n",
    "\n",
    "* Randomization allows us to avoid pathalogical orderings of the data that might cause us to go off in the wrong direction.\n",
    "\n",
    "* If we are solving a non-convex problem (e.g. not this case), then randomization can also help to avoid and escape local minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Latex\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our minimization problem is\n",
    "$$\n",
    "\\min_\\theta \\mathcal{L}\n",
    "$$\n",
    "where our loss function is\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{m} \\|X\\theta - y\\|^2 + \\lambda\\|\\theta\\|^2\n",
    "$$\n",
    "\n",
    "The gradient is \n",
    "$$\n",
    "\\nabla_{\\theta} \\mathcal{L} = \\frac{2}{m} X^T (X\\theta - y) + 2\\lambda\\theta\n",
    "$$\n",
    "\n",
    "If we designate our error term as\n",
    "$$\n",
    "\\hat{\\varepsilon} = X\\theta - y\n",
    "$$\n",
    "then we have\n",
    "$$\n",
    "\\nabla_{\\theta} \\mathcal{L} = \\frac{2}{m} X^T \\hat{\\varepsilon} + 2\\lambda\\theta\n",
    "$$\n",
    "\n",
    "This formulation corresponds directly to the implementations below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On CPU using Pandas\n",
    "\n",
    "Pandas + Numpy is often easiest for smaller problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "# Generate synthetic dataset\n",
    "n_samples = 800\n",
    "n_features = 50_000\n",
    "\n",
    "# Features\n",
    "X = pd.DataFrame(np.random.randn(n_samples, n_features))\n",
    "X = (X - X.mean()) / X.std()\n",
    "\n",
    "# True weights\n",
    "true_weights = np.random.randn(n_features)\n",
    "\n",
    "# Target variable with noise\n",
    "y = pd.Series(X.values @ true_weights + 0.1 * np.random.randn(n_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 36228.0376\n",
      "Epoch 2, Loss: 25689.1424\n",
      "Epoch 3, Loss: 18248.4911\n",
      "Epoch 4, Loss: 12984.1272\n",
      "Epoch 5, Loss: 9254.2042\n",
      "Epoch 6, Loss: 6607.6020\n",
      "Epoch 7, Loss: 4725.7599\n",
      "Epoch 8, Loss: 3385.4724\n",
      "Epoch 9, Loss: 2429.2009\n",
      "Epoch 10, Loss: 1746.0195\n",
      "Epoch 11, Loss: 1256.9572\n",
      "Epoch 12, Loss: 906.2954\n",
      "Epoch 13, Loss: 654.4701\n",
      "Epoch 14, Loss: 473.3479\n",
      "Epoch 15, Loss: 342.8702\n",
      "Epoch 16, Loss: 248.7302\n",
      "Epoch 17, Loss: 180.6747\n",
      "Epoch 18, Loss: 131.4334\n",
      "Epoch 19, Loss: 95.7505\n",
      "Epoch 20, Loss: 69.8447\n",
      "Epoch 21, Loss: 51.0148\n",
      "Epoch 22, Loss: 37.3107\n",
      "Epoch 23, Loss: 27.3199\n",
      "Epoch 24, Loss: 20.0294\n",
      "Epoch 25, Loss: 14.7016\n",
      "Epoch 26, Loss: 10.8026\n",
      "Epoch 27, Loss: 7.9472\n",
      "Epoch 28, Loss: 5.8531\n",
      "Epoch 29, Loss: 4.3154\n",
      "Epoch 30, Loss: 3.1850\n",
      "Epoch 31, Loss: 2.3531\n",
      "Epoch 32, Loss: 1.7402\n",
      "Epoch 33, Loss: 1.2883\n",
      "Epoch 34, Loss: 0.9546\n",
      "Epoch 35, Loss: 0.7081\n",
      "Epoch 36, Loss: 0.5257\n",
      "Epoch 37, Loss: 0.3907\n",
      "Epoch 38, Loss: 0.2906\n",
      "Epoch 39, Loss: 0.2164\n",
      "Epoch 40, Loss: 0.1613\n",
      "Epoch 41, Loss: 0.1203\n",
      "Epoch 42, Loss: 0.0898\n",
      "Epoch 43, Loss: 0.0672\n",
      "Epoch 44, Loss: 0.0503\n",
      "Epoch 45, Loss: 0.0376\n",
      "Epoch 46, Loss: 0.0282\n",
      "Epoch 47, Loss: 0.0212\n",
      "Epoch 48, Loss: 0.0159\n",
      "Epoch 49, Loss: 0.0120\n",
      "Epoch 50, Loss: 0.0090\n",
      "Training completed in 2.47 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialize weights\n",
    "weights = np.zeros(n_features)\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Timing start\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "  # Shuffle the data\n",
    "  indices = np.random.permutation(n_samples)\n",
    "  X_shuffled = X.values[indices]\n",
    "  y_shuffled = y.values[indices]\n",
    "\n",
    "  for i in range(0, n_samples, batch_size):\n",
    "    X_batch = X_shuffled[i:i+batch_size]\n",
    "    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "    # Predict\n",
    "    y_pred = X_batch @ weights\n",
    "\n",
    "    # Compute error\n",
    "    error = y_pred - y_batch\n",
    "\n",
    "    l2_lambda = 1e-3\n",
    "\n",
    "    # Compute gradient\n",
    "    gradient = X_batch.T @ error / batch_size + l2_lambda * weights\n",
    "\n",
    "    # Update weights\n",
    "    weights -= learning_rate * gradient\n",
    "\n",
    "  # Compute loss\n",
    "  y_full_pred = X.values @ weights\n",
    "  loss = np.mean((y_full_pred - y.values) ** 2)\n",
    "  print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Timing end\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by ANOVA\n",
      "15970    0.436115\n",
      "14857    0.417547\n",
      "21647    0.403001\n",
      "34785    0.395743\n",
      "39648    0.392069\n",
      "47028    0.387484\n",
      "10164    0.384219\n",
      "2283     0.379538\n",
      "5623     0.379386\n",
      "14309    0.376675\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_mean = X.mean(axis=0).values\n",
    "anova = (X - X_mean) * weights  # shape (n_samples, n_features)\n",
    "\n",
    "print(\"Top 10 features by ANOVA\")\n",
    "print(\n",
    "  anova\n",
    "  .abs()\n",
    "  .mean(axis=0)\n",
    "  .sort_values(ascending=False).head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PyTorch\n",
    "\n",
    "Pytorch makes it really easy to parallize onto GPUs or using the parallel processing capabilities of Apple processors (MPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Epoch 1, Loss: 23945.5039\n",
      "Epoch 2, Loss: 11302.5322\n",
      "Epoch 3, Loss: 5380.1025\n",
      "Epoch 4, Loss: 2583.7488\n",
      "Epoch 5, Loss: 1251.6373\n",
      "Epoch 6, Loss: 611.0913\n",
      "Epoch 7, Loss: 300.6081\n",
      "Epoch 8, Loss: 148.9720\n",
      "Epoch 9, Loss: 74.3519\n",
      "Epoch 10, Loss: 37.3445\n",
      "Epoch 11, Loss: 18.8742\n",
      "Epoch 12, Loss: 9.5935\n",
      "Epoch 13, Loss: 4.9027\n",
      "Epoch 14, Loss: 2.5185\n",
      "Epoch 15, Loss: 1.3005\n",
      "Epoch 16, Loss: 0.6748\n",
      "Epoch 17, Loss: 0.3517\n",
      "Epoch 18, Loss: 0.1841\n",
      "Epoch 19, Loss: 0.0969\n",
      "Epoch 20, Loss: 0.0512\n",
      "Epoch 21, Loss: 0.0272\n",
      "Epoch 22, Loss: 0.0145\n",
      "Epoch 23, Loss: 0.0078\n",
      "Epoch 24, Loss: 0.0042\n",
      "Epoch 25, Loss: 0.0023\n",
      "Epoch 26, Loss: 0.0013\n",
      "Epoch 27, Loss: 0.0007\n",
      "Epoch 28, Loss: 0.0004\n",
      "Epoch 29, Loss: 0.0002\n",
      "Epoch 30, Loss: 0.0001\n",
      "Epoch 31, Loss: 0.0001\n",
      "Epoch 32, Loss: 0.0001\n",
      "Epoch 33, Loss: 0.0000\n",
      "Epoch 34, Loss: 0.0000\n",
      "Epoch 35, Loss: 0.0000\n",
      "Epoch 36, Loss: 0.0000\n",
      "Epoch 37, Loss: 0.0000\n",
      "Epoch 38, Loss: 0.0000\n",
      "Epoch 39, Loss: 0.0000\n",
      "Epoch 40, Loss: 0.0000\n",
      "Epoch 41, Loss: 0.0000\n",
      "Epoch 42, Loss: 0.0000\n",
      "Epoch 43, Loss: 0.0000\n",
      "Epoch 44, Loss: 0.0000\n",
      "Epoch 45, Loss: 0.0000\n",
      "Epoch 46, Loss: 0.0000\n",
      "Epoch 47, Loss: 0.0000\n",
      "Epoch 48, Loss: 0.0000\n",
      "Epoch 49, Loss: 0.0000\n",
      "Epoch 50, Loss: 0.0000\n",
      "Training completed in 6.03 seconds\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import timeit\n",
    "\n",
    "# Select device (GPU if available, else CPU)\n",
    "device = (\n",
    "  torch.device(\"cuda\")\n",
    "  if torch.cuda.is_available()\n",
    "  else torch.device(\"mps\")\n",
    "  if torch.backends.mps.is_available()\n",
    "  else torch.device(\"cpu\")\n",
    ")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "X_tensor = torch.tensor(X.values.astype(np.float32), device=device)\n",
    "y_tensor = torch.tensor(y.values.astype(np.float32), device=device)\n",
    "\n",
    "# Initialize model parameters\n",
    "weights = torch.zeros(n_features, device=device, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "n_epochs = 50\n",
    "batch_size = 32\n",
    "l2_lambda = 1e-3\n",
    "\n",
    "# Timing start\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "  perm = torch.randperm(n_samples, device=device)\n",
    "  X_shuffled = X_tensor[perm]\n",
    "  y_shuffled = y_tensor[perm]\n",
    "\n",
    "  for i in range(0, n_samples, batch_size):\n",
    "    X_batch = X_shuffled[i:i+batch_size]\n",
    "    y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "    y_pred = X_batch @ weights\n",
    "    error = y_pred - y_batch\n",
    "\n",
    "    loss = (error ** 2).mean() + l2_lambda * (weights ** 2).sum()\n",
    "\n",
    "    # Backpropagation - this calculates the gradients for all the tensors with requires_grad=True\n",
    "    loss.backward()\n",
    "\n",
    "    # SGD step - this is where we update the weights\n",
    "    with torch.no_grad():\n",
    "      weights -= learning_rate * weights.grad\n",
    "      weights.grad.zero_()\n",
    "\n",
    "  # Compute full loss\n",
    "  with torch.no_grad():\n",
    "    y_full_pred = X_tensor @ weights\n",
    "    full_loss = ((y_full_pred - y_tensor) ** 2).mean().item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {full_loss:.4f}\")\n",
    "\n",
    "# Timing end\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALthough the total time is longer, it converges much faster (presumably because it uses better gradients), so if we were stopping on an epsilon, it would probably be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by ANOVA\n",
      "15970    0.436277\n",
      "14857    0.417662\n",
      "21647    0.403175\n",
      "34785    0.395839\n",
      "39648    0.392215\n",
      "47028    0.387597\n",
      "10164    0.384388\n",
      "2283     0.379671\n",
      "5623     0.379494\n",
      "14309    0.376724\n",
      "dtype: float32\n"
     ]
    }
   ],
   "source": [
    "X_tensor_mean = X_tensor.mean(axis=0)\n",
    "anova_torch = (X_tensor - X_tensor_mean) * weights  # shape (n_samples, n_features)\n",
    "\n",
    "print(\"Top 10 features by ANOVA\")\n",
    "print(\n",
    "  pd.Series(\n",
    "    anova_torch\n",
    "    .abs()\n",
    "    .mean(axis=0)\n",
    "    .cpu()\n",
    "    .detach()\n",
    "    .numpy()\n",
    "  )\n",
    "  .sort_values(ascending=False)\n",
    "  .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pseudo inverse via SVD with L2 regularization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss (SVD pseudo-inverse): 0.0000\n",
      "Training completed in 3.73 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Timing start\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# Compute SVD\n",
    "U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "\n",
    "# Compute pseudo-inverse of the singular values\n",
    "l2_lambda = 1e-3  # Same as other methods\n",
    "S_inv = np.diag(S / (S**2 + l2_lambda))  # Regularized version\n",
    "\n",
    "# Compute pseudo-inverse of X\n",
    "X_pinv = Vt.T @ S_inv @ U.T\n",
    "\n",
    "# Solve for weights\n",
    "w_svd = X_pinv @ y\n",
    "\n",
    "# Timing end\n",
    "end_time = timeit.default_timer()\n",
    "\n",
    "# Check training loss\n",
    "y_pred = X @ w_svd\n",
    "loss = np.mean((y_pred - y) ** 2)\n",
    "print(f\"Training Loss (SVD pseudo-inverse): {loss:.4f}\")\n",
    "\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by ANOVA\n",
      "15970    0.436282\n",
      "14857    0.417668\n",
      "21647    0.403180\n",
      "34785    0.395845\n",
      "39648    0.392221\n",
      "47028    0.387603\n",
      "10164    0.384394\n",
      "2283     0.379676\n",
      "5623     0.379500\n",
      "14309    0.376729\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "anova_svd = (X - X_mean) * w_svd  # shape (n_samples, n_features)\n",
    "\n",
    "print(\"Top 10 features by ANOVA\")\n",
    "print(\n",
    "  anova_svd\n",
    "  .abs()\n",
    "  .mean(axis=0)\n",
    "  .sort_values(ascending=False).head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
